![](../架构设计/.注册中心_images/注册中心选型.png)

# etcd

etcd定位是个低容量的关键元数据存储，db大小一般不超过8g。存储引擎和API上Redis内存实现了各种丰富数据结构，而etcd仅是kv API, 使用的是持久化存储boltdb

# 读流程

读请求从 client 通过 Round-robin 负载均衡算法, 选择一个 etcd server 节点, 发出 grpc 请求, 经过 etcd server 的 KVSever 模块、线性读模块、MVCC 的 treeIndex 和
boltdb 模块紧密协作

## 架构图

![](../架构设计/.注册中心_images/229709c8.png)

1. client层: 负载均衡、节点故障自动转移
2. API 网络层: client 访问 server 的通信协议, server 之间的通信协议, 指节点间通过 raft 算法实现数据复制和 Leader 选举
3. raft 算法层: leader选举、日志复制、readIndex 核心算法特性, 保障 etcd 多个节点数据一致性、提升服务可用性
4. 功能逻辑层: KVServer 模块(metrics、日志、请求行为检查)、MVCC 模块、Auth 鉴权模块、Lease 租约模块、Compactor 压缩模块, MVCC 模块由 treeIndex 模块和 boltdb 模块组成
5. 存储层: 包含预写(WAL)模块、快照模块、boltdb 模块, wal 保障 etcd crash 后数据不丢失, boltdb 保存了集群元数据和用户写入数据

## MVCC

更新K-V数据不会覆盖原来数据, 而是新增版本来覆盖元数据, treeIndex 模块基于内存版 B-tree 实现 key 索引管理, 保存用户 key 和版本号的映射关系

boltdb 是基于 B+ tree 实现的 k-v 键值库, 支持事务, 提供简易 API 给 etcd 操作, etcd 如何基于 boltdb 保存 key 多个历史版本

查询默认返回最新版本, 带上版本号就返回版本数据

删除是先打标记, 然后异步删除, 防止导致树不平衡, 反复调整, 影响读写性能

可以理解为这个revision就是一个全局的ID,表示已经执行了多少次操作, 每一次操作都有唯一的revision来识别, 对于内存中的B树来说, 它在进行查找时所使用的search-key是etcd key,
节点中存储的就是revision信息.而硬盘中存储的B+树的search-key就是revision值, 其节点中存储的是etcd key和etcd value, 通过这样的组织结构, etcd做到了保存每一个key 的每一个历史记录

### 为什么用 B-tree

为什么不用哈希表、平衡二叉树?
从 etcd 功能特性分析, etcd 支持范围查询, hash 索引不是按照索引列顺序存储的, 无法排序, hash 冲突的时候要取出所有的行指针
从性能分析, 平衡二叉树每个节点智能容纳一个数据, 导致树的高度较高, 而 B-tree 每个节点可以容纳多个数据, 树的高度更低, 更扁平, 涉及查找次数更少

### 方案一: 一个 key 保存多个历史版本

导致 value 较大, 存在读写放大、并发冲突问题

### 方案二: 每次修改操作生成一个新的版本号, 版本号作为 key, value 是 key-value

etcd 采用此方案, boltdb 的 key 是全局递增的版本号, value 是用户的 key-value 等字段组合的结构体, 通过 treeIndex 保存用户 key 和版本号对应的关系
treeIndex 与 boltdb 关系是从 treeIndex 获取 key hello 版本号, 再以版本号作为 boltdb 的 key, 从 boltdb 获取其 value 信息
![](../架构设计/.注册中心_images/52a0794d.png)

### readIndex

1. 读到旧数据, Follower 收到 Leader 节点同步的写请求后, 怎么保证最新的数据应用到状态机, 这个机制叫 readIndex
2. 收到读请求时, 先从 Leader 获取集群最新提交日志索引(committed index), Leader 收到 readIndex 请求为了防止脑裂, 向 Follower 节点发送心跳确认,
   一半以上确认 Leader 身份后才能将已提交的索引返回给节点 C, C节点等状态机应用索引超过提交索引(4过程),
3. 通知读请求, 数据赶上 Leader, 去状态机访问数据
   ![](../架构设计/.注册中心_images/30d8b3ab.png)

### treeIndex

treeIndex 模块是基于 google 开源的 btree 实现, treeIndex 只保存用户 key 和版本号信息, 相比 zookeeper 全内存存储, etcd 对内存要求更低

### buffer

获取到版本号信息后, 可从 boltdb 模块中获取用户的 key-cakue 数据, etcd 出于一致性、性能考虑, 访问 boltdb 前, 首先会从内存读事务 buffer 中, 二分查找 key 是否在 buffer 里面,
命中直接返回

# boltdb

1. 如果 buffer 未命中, 就去 boltdb 查询, 此时真正需要向 boltdb 模块查询数据库
2. mysql 通过 table 隔离, boltdb 就是通过 bucket 隔离, etcd MVCC 元数据存放的 bucket 是 meta
3. 因为 boltdb 使用 B+ tree 组织用户的 key-value 数据, 获取 bucket key 对象后, 通过 boltdb 游标 Cursor 快速在 B+ tree 查找 key hello 对应的 value 数据

![](../架构设计/.注册中心_images/fca3be87.png)

# 写流程

![](../架构设计/.注册中心_images/b2d08daa.png)

1. client 负载均衡算法选择一个 etcd server 节点, 发起 gRPC 调用
2. etcd 节点收到请求后经过 gRPC 拦截器、Quota模块后, 进入 KVServer 模块, KVServer 模块向 Raft 模块提案
3. 提案通过 RaftHTTP 网络模块转发、集群多数节点持久化后, 状态变为已提交, etcdserver 从 Raft 模块获取已提交的日志, 传递给 Apply 模块, Apply 模块通过 MVCC 模块执行提案内容, 更新状态机
4. 写流程还经过 Quota、WAL、Apply 三个模块, crash-safe 及幂等性也是 WAL 和 Apply 流程的 consistent index 实现的, key-value数据与consistent
   index在同一个boltdb事务中更新

## Quota 模块

1. etcd db 文件大小超过配额, 出现此错误后, 整个集群将不可写入, 只读对业务影响非常大, 因为 MVCC 数据库保存 key 历史版本, 数据不断写入会超过大小
2. 触发 Quota 限制后, 写入会检查 etcd db 大小加上请求的 key-value 大小之和是否超过了配额

## KVServer

KVServer 为了保证集群稳定, 避免雪崩, 任何提交到 Raft 模块请求, 会做简单的限速判断, Raft 模块提交的日志索引比已经应用到的状态机日志索引超过 5k, 返回异常

![](../架构设计/.注册中心_images/904ef9f0.png)

## Propose

通过一系列检查之后, 生成一个唯一 ID, 将此请求关联到 channel, 向 Raft 模块发起提案, KYServer 等待 put 请求, 写入结果超时

## WAL 模块

Raft 模块收到提案, 当前节点是 Follower, 转发给 Leader, server 获取消息后, 广播给集群各节点, 把集群 Leader 任期号、投票信息、提交索引、提案内容持久化到 WAL

## Apply 模块

![](../架构设计/.注册中心_images/7ba903bd.png)

1. 提交给 Apply 模块的提案获得多数节点的确认, 从 Raft 模块获得日志条目信息, 是否由唯一字段, 如果没有会导致一个命令执行多次, 系统的一致性状态无法保证, 导致数据混乱
2. Raft 日志条目中索引(index)字段, 命令执行成功后全局单调递增 index, 但是不够安全, 如果执行命令的请求成功了, 更新 index 失败了
3. 必须保证原子性提交、consistent index 字段, 来存储系统当前已经执行过的日志条目索引, 实现幂等

## treeIndex

![](../架构设计/.注册中心_images/fc8ef384.png)

## boltdb

1. 事务提交过程包含 B+ tree 的平衡、分裂, 将 boltdb 脏数据(dirty page)、元数据信息刷盘, 事务提交开销很昂贵, 解决办法是合并在合并
2. 异步机制将事务一次性提交, 提高吞吐量, 引入 bucket buffer 保存暂未提交的事务数据, 更新 boltdb 时, etcd 会同步数据到 bucket buffer, etcd 处理数据会优先从 bucket
   buffer 读取, 再从 boltdb 读

# 不一致

## 开启 etcd 数据损坏检测功能

枚举 boltdb 文件的内容跟其他节点内容是否一致, 遍历 treeIndex 模块所有的 key 获取到版本号, 根据版本号从 boltdb 获取 key 的 value

## 应用层数据一致性检测

数据不一致在 MVCC、boltdb 会出现很多种情况, key数量不一致、etcd 逻辑时钟版本号不一致, MVCC 模块收到put操作metrics 指标值不一致, 定时统计 key 数量, endpoint 各节点 revision
信息做一致性监控, 基于 etcd MVCC 的 metrics 指标监控

## 定时备份数据

etcd 数据不一致修复工作机器棘手, 各个节点都可能包含新数据和脏数据, 只能用备份数据来恢复, 做任何重要变更之前先备份数据, 在生产环境增加定期数据备份机制
使用 etcd-operator 中的 backup-operator 实现定时数据备份

## 良好的运维规范(使用比较稳定的版本、确保版本一致性、灰度变更)

确保各个节点 etcd 版本一致, 逻辑差异性会增大 BUG 概率, 优先使用新而稳定的版本 etcd, 升级 etcd 版本的时候, 多查看 change log, 评估是否存在可能有不兼容的特性, 生产环境先灰度、再全量