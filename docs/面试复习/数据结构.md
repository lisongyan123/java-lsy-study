# 技能树

![](../.数据结构_images/9e764ed9.png)

# 线性表

## 数组

## 链表

### 淘汰策略

1. FIFO
2. 最少使用 LFU
3. 最近最少使用 LRU

## 栈

### 如何实现浏览器前进后退的功能

两个栈 X 和 Y
面进入栈 X, 后退把页面从 X 出栈, 放入Y 栈, 前进就访问 Y 栈, 回过头在看可以用加入 X 栈访问, 此时页面 X 无法前进

## 跳跃表

![](../.数据结构_images/d434b286.png)
第 k 级索引的结点个数是第 k-1 级索引的结点个数的 1/2,那第 k级索引结点的个数就是 n/(2k)

### redis 为什么用跳跃表不用红黑树

redis 还用了散列表,具有插入、删除、查找、迭代输出、有序序列,红黑树也可以完成但是区间查找效率很低,跳跃表可以有效平衡执行效率和内存消耗

## 散列表

word 标红拼写错误,使用散列表寻找, 将单词的每个字母 ASCII 码值仅为相加, 跟散列表的大小求余、取模、作为散列值

一堆中文中匹配一个中文是否存在, 用散列表

### 场景题目

假设猎聘网有 10 万名猎头,每个猎头都可以通过做任务(比如发布职位)来积累积分,然后通过积分来下载简历。假设你是猎聘网的一名工程师,如何在内存中存储这
10 万个猎头 ID 和积分信息

1. ID 在散列表中所以可以 O(1) 查找到这个猎头
2. 积分以跳表存储,跳表支持区间查询
3. 查找按照积分从小到大排名在第 x 位到第 y 位之间的猎头 ID 列表目前无法实现

## 链表数组优化

1. 数组占据随机访问的优势,却有需要连续内存的缺点
2. 链表具有可不连续存储的优势,但访问查找是线性的
3. 散列表和链表、跳表的混合使用,是为了结合数组和链表的优势,规避它们的不足

## 图形结构

图的搜索、最短路径、最小生成树、二分图, 社交 Graph 在推荐系统中应用非常广泛

### 场景

**微博**

1. 判断用户 A 是否关注了用户 B
2. 判断用户 A 是否是用户 B 的粉丝
3. 用户 A 关注用户 B
4. 用户 A 取消关注用户 B
5. 根据用户名称的首字母排序, 分页获取用户的粉丝列表
6. 根据用户名称的首字母排序, 分页获取用户的关注列表

**存储规模**

1. 使用邻接表存储粉丝关系
2. 逆邻接表存储用户被关注关系
3. 海量用户的话, hash + 邻接表, 分片存储

快速知道谁关注了谁, 可以用 hash 算法 + 数据分片的形式, 邻接表存储在不同机器

## Trie 树 Apache Commons 有开源实现

Trie 树只是不适合精确匹配查找, 散列表或者红黑树来解决精确匹配比较好

拓展还有双数组trie树

## AC 自动机

过滤掉用户输入的一些淫秽、反动、谩骂, 用“***”把它替代掉

KMP + Trie 树

1. BF: 简单场景,主串和模式串都不太长, O(m*n)
2. KP: 字符集范围不要太大且模式串不要太长, 否则 hash 值可能冲突,O(n)
3. naive-BM: 模式串最好不要太长(因为预处理较重),比如 IDE 编辑器里的查找场景, 处理O(m*m), 匹配O(n), 实现较复杂,需要较多额外空间
4. KMP: 适合所有场景,整体实现起来也比 BM 简单,O(n+m),仅需一个next数组的O(n)额外空间, 但统计意义下似乎 BM 更快,原因不明
5. 另外查资料的时候还看到一种比BM/KMP更快,且实现+理解起来都更容易的的Sunday算法
6. naive-Trie: 适合多模式串公共前缀较多的匹配 (O(n*k)) 或者 根据公共前缀进行查找 (O(k)) 的场景,比如搜索框的自动补全提示
7. AC 自动机: 适合大量文本中多模式串的精确匹配查找, 可以到O(n)

## bitmap

展示某一个月的签到情况, 用 bitmap 日期 offset = hash % 365; key = 年份#用户id 统计活跃用户, 使用时间作为 cacheKey,
然后用户ID为 offset, 如果当日活跃过就设置为 1

### roaring bitmap

交集或并集运算, 比 bitmap 更节省空间

Roaring Bitmap 将一个 32 位的整数分为两部分, 一部分是高 16 位, 另一部分是低 16 位。对于高 16 位, Roaring Bitmap
将它存储到一个有序数组中, 这个有序数组中的每一个值都是一个“桶”; 而对于低 16 位, Roaring Bitmap 则将它存储在一个 2^16
的位图中, 将相应位置置为 1。每个桶都会对应一个 2^16 的位图

## Bloom Filter

基于位图, 爬虫去重, cpu 多次 hash 计算

# 算法

![](../.数据结构_images/b455a9ba.png)

## 桶排序

![](../.数据结构_images/0e4a533f.png)

1. 要排序的数据需要很容易就能划分成 m 个桶
2. 桶与桶之间有着天然的大小顺序。这样每个桶内的数据都排序完之后,桶与桶之间的数据不需要再进行排序
3. 避免数据倾斜

### 使用场景

外部磁盘, 数据流比较大内饰有限

## 计数排序

当要排序的 n 个数据,所处的范围并不大的时候,比如最大值是 k,我们就可以把数据划分成 k 个桶, 考生分数分为 901 个桶, 算出 900
分制, 每个用户的桶

## hash 算法

将任意长度的二进制值串映射为固定长度的二进制值串,这个映射的规则就是哈希算法, 不能反向推导出原始数据

### 加密算法

1. MD5(MD5 Message-Digest Algorithm,MD5 消息摘要算法)不算加密
    1. MD5 可以用彩虹表破解, 盐不要写死, 因为“salt” + password 虽然无法被破解, 但是黑客自己注册一个账号密码为 11111
       很容易破解出原来的密码, 最好每个独立的密码都有盐, 20位, 可以使用 UUID 作为盐, 防止盐被做成彩虹表
2. SHA(Secure Hash Algorithm,安全散列算法)
3. 对称加密: 加密速度快, 但密钥容易被窃取, 敏感数据都是服务端的可以使用 3DES、DES(Data Encryption Standard,数据加密标准)
   AES(Advanced Encryption Standard,高级加密标准)，推荐后者
4. 非对称加密: 公钥公开, 私钥不公开, 加密速度慢但是解决密钥分发安全问题 rsa,ecc

#### 场景

1. 如何存储用户密码这么重要的数据, 字典加 salt, 和密码组合在一起, 增加密码复杂度
    1. 大部分公司采用 PBKDF2WithHmacSHA1 计算足够慢
2. BT 下载防止网络被攻击, hash 值将下载好的文件块逐一对比, 一样的话最后算出来的结果不同
3. 姓名电话身份证
4. 区块链
    1. 区块链是一块块区块组成的,每个区块分为两部分: 区块头和区块体
    2. 区块头保存着 自己区块体 和 上一个区块头 的哈希值
    3. 因为这种链式关系和哈希值的唯一性, 只要区块链上任意一个区块被修改过, 后面所有区块保存的哈希值就不对了
    4. 区块链使用的是 SHA256 哈希算法, 计算哈希值非常耗时, 如果要篡改一个区块, 就必须重新计算该区块后面所有的区块的哈希值,短时间内几乎不可能做到
5. etcd 鉴权认证
   1. 随机加盐、高安全性的 hash 函数、自定义 hash 值计算迭代次数 cost
   2. etcd 密码存储, 使用 bcrpt 库的 blowfish 算法, 基于铭文密码、随机分配 salt、自定义 cost、迭代多次计算得到一个 hash 值, 将加密算法版本、salt值、cost、hash 值组成一个字符串, 作为加密后的密码

#### 提升密码性能

1. 避免频繁、昂贵的密码计算匹配
2. 返回一个通信证凭据给 client, 后续请求携带通信证, token

## BF(Brute Force) 算法

![](../.数据结构_images/107da048.png)

字符串匹配

## RK(Rabin-Karp) 算法

![](../.数据结构_images/0867a096.png)

基于 hash + BF 的字符串匹配

## BM(Boyer-Moore) 算法

高效替换的算法

## 搜索引擎背后算法

1. 搜集: 爬虫爬取网页
2. 分析: 负责网页内容抽取、分词, 构建临时索引, 计算 PageRank 值
    1. 去掉 JavaScript 代码、CSS 格式以及下拉框中的内容, 利用 AC 自动机这种多模式串匹配算法
    2. 利用 AC 自动机这种多模式串匹配算法
    3. 把单词与网页之间的对应关系,写入到一个临时索引文件中(tmp_Index.bin)
    4. 维护一个计数器term_id, 所有的网页处理(分词及写入临时索引)完成之后,我们再将这个单词跟编号之间的对应关系,写入到磁盘文件中,并命名为
       term_id.bin
3. 索引: 主要负责通过分析阶段得到的临时索引,构建倒排索引
    1. 倒排索引, 每个单词编号在倒排索引文件中的偏移位置 term_offset.bin, 倒排索引文件(index.bin)
4. 查询: 响应用户的请求,根据倒排索引获取相关网页,计算网页排名,返回查询结果给用户
    1. doc_id.bin: 记录网页链接和编号之间的对应关系
    2. term_id.bin: 记录单词和编号之间的对应关系
    3. index.bin: 倒排索引文件,记录每个单词编号以及对应包含它的网页编号列表
    4. term_offsert.bin: 记录每个单词编号在倒排索引文件中的偏移位置
    5. 先对用户输入的文本进行分词处理，假设分分词之后, 我们得到 k 个单词
    6. 拿这 k 个单词,去 term_id.bin 对应的散列表中, 查找对应的单词编号
    7. 去 term_offset.bin 对应的散列表中,查找每个单词编号在倒排索引文件中的偏移位置
    8. 去倒排索引(index.bin)中,查找 k 个单词对应的包含它的网页编号列表, 经过这一步查询之后,我们得到了 k 个网页编号列表
    9. 针对这 k 个网页编号列表,统计每个网页编号出现的次数, 按照出现次数的多少,从小到大排序。出现次数越多,说明包含越多的用户查询单词
    10. 得到了一组排好序的网页编号。我们拿着网页编号,去 doc_id.bin 文件中查找对应的网页链接,分页显示给用户就可以了
    11. 计算网页权重的 PageRank 算法、计算查询结果排名的 tf-idf 模型

## 短 url

新的短网址生成的时候, 我们先拿这个新生成的短网址, 在布隆过滤器中查找

## ID 生成

通过 ID 生成, 给 ID 生成器装多个前置发号器。我们批量地给每个前置发号器发送 ID 号码。当我们接受到短网址生成请求的时候,就选择一个前置发号器来取号码
![](../.数据结构_images/95a11368.png)
